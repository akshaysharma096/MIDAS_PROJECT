{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier - MIDAS INTERNSHIP CHALLENGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation - 1\n",
    "The following code is used to import the our 3rd Party Deep Learning Libraries like *Pytorch* as well as checks, if the GPU is set or not.\n",
    "\n",
    "<img src =\"https://cdn-images-1.medium.com/max/2600/1*aqNgmfyBIStLrf9k7d9cng.jpeg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is usage: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import torchvision\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(\"Device is usage: {0}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation 2\n",
    "\n",
    "### Intuition\n",
    "The code block below defines few global variables that we would like to take care of, for example the batch size for our Deep Learning Pipeline as well as the paths of the training data/labels as well as the test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\"\n",
    "train_images_path = \"{0}/train_image.pkl\".format(data_path)\n",
    "test_images_path = \"{0}/test_image.pkl\".format(data_path)\n",
    "train_labels_path = \"{0}/train_label.pkl\".format(data_path)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def path_exists(path):\n",
    "    \"\"\"\n",
    "     Function to verfiy if, file path defined is correct\n",
    "    \"\"\"\n",
    "    return os.path.exists(path)\n",
    "\n",
    "valid_path = [path_exists(_) for _ in (train_images_path, test_images_path, test_images_path)]\n",
    "valid_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Pipeline - Data Pre-processing\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*ZX05x1xYgaVoa4Vn2kKS9g.png\" />\n",
    "\n",
    "### What's happening ?\n",
    "In the code block we have defined a basic class which takes care of our pre-processing pipeline.\n",
    "\n",
    "#### Why did we **inherit** the `Dataset`  class ?\n",
    "We are using PyTorch as our DeepLearning framework, PyTorch provides a very simple API to build the Deep Learnign PipeLine, we can do make our own pipeline by inheriting the **Dataset** class, which has internal methods to take of the things we need to take care of pre-processing.\n",
    "\n",
    "Here in our case I have divided the pipelin in the following parts.\n",
    "- Load data from pickle dump.\n",
    "- Build an API to *clean the data*, or arrange the data in a format that PyTorch understands, this has been done by the `__getitem__()` function.\n",
    "    - This functions iteratively returns a single dataset and the following label for our Neural Net to work on. \n",
    "- We also incorporate transformations in the class, which helps us normalising our data.\n",
    "    - **Normalising** input data is important, as this helps *Gradient Descent* to run faster, which means we can use a higher learning rate for our Neural Network, this helps us the reacht the minima faster, the contour of the cost functions are symmetrical when data is normalised.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetLoader(Dataset):\n",
    "    '''Dataset Loader'''\n",
    "    def __init__(self, train_path, labels_path, transform=None, train=True, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_path (string): Path to the training data file\n",
    "            labels_path (string): Path to the labels present for the training data\n",
    "            transform (callable): Optional transform to apply to sample\n",
    "        \"\"\"\n",
    "        self.train=train\n",
    "        if self.train:\n",
    "            data = self._load_from_pickle(train_path)\n",
    "            self.train_data = torch.ByteTensor(data).view(-1, 28, 28) \n",
    "            self.train_labels = self._load_from_pickle(labels_path)\n",
    "        else:\n",
    "            data = self._load_from_pickle(train_path)\n",
    "            self.test_data = torch.ByteTensor(data).view(-1, 28, 28) \n",
    "            self.test_lables = []\n",
    "            \n",
    "        del data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "         Returns the length of whole Dataset fed into the Neural Net.\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "         Returns a single training/test example after applying the required normalisation/transformations techniques.\n",
    "         As well as the label.\n",
    "         \n",
    "         ret: (image, label)\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            img, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            img, target = self.test_data[index], self.test_labels[index]\n",
    "\n",
    "        # return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        return img, target\n",
    "    \n",
    "    def _load_from_pickle(self, file_path):\n",
    "        \"\"\"\n",
    "         file_path: File path to load data, returns an ndarray\n",
    "         \n",
    "         ret: Loaded dump, in primitive data format.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "        return data\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"\n",
    "         Return an array of labels present in the data set.\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            return np.unique(self.train_labels)\n",
    "        return []\n",
    "    \n",
    "    def visualise_data_set(self):\n",
    "        \"\"\"\n",
    "         Utility function to randomly display images from the dataset\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        columns = 4\n",
    "        rows = 5\n",
    "        if self.train:\n",
    "            data_set = self.train_data\n",
    "        else:\n",
    "            data_set = self.test_data\n",
    "        for i in range(1, columns * rows +1):\n",
    "            img_xy = np.random.randint(len(data_set));\n",
    "            img = data_set[img_xy][0][0,:,:]\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.title(labels_map[data_set[img_xy][1]])\n",
    "            plt.axis('off')\n",
    "            plt.imshow(img)\n",
    "        \n",
    "        plt.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a labels map for storing the labels\n",
    "\n",
    "\n",
    "# Define the normalisation/tranformation techniques we need during pre-processing.\n",
    "img_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test data sets.\n",
    "train_dataset = DataSetLoader(train_images_path, train_labels_path, train=True, transform=img_transforms)\n",
    "test_dataset = DataSetLoader(test_images_path, \"\", train=False, transform=img_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data\n",
    "\n",
    "The following code blocks gives us some insights about what kind of images our data set has.\n",
    "By looking at the training set, we have 4 labels, *0, 2 ,3, 6*, these labels are numerically labelled in our training labelled data set.\n",
    "\n",
    "Just for readibilty I have created a dictionary of mapping each numerical label to a physical label, like *Shirt*, *T-Shirt* etc.\n",
    "\n",
    "```\n",
    "labels = train_dataset.get_labels()\n",
    "```\n",
    "The code above gives us the unique labels present in our training data.\n",
    "```\n",
    "labels_map = {0 : 'T-Shirt', 2 : 'Pullover', 3 : 'Dress', 6 : 'Shirt'}\n",
    "```\n",
    "Initialises a dictionary to map each numerical label to a physical entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0, value: T-Shirt\n",
      "Label: 2, value: Pullover\n",
      "Label: 3, value: Dress\n",
      "Label: 6, value: Shirt\n"
     ]
    }
   ],
   "source": [
    "labels = train_dataset.get_labels()\n",
    "labels_map = {0 : 'T-Shirt', 2 : 'Pullover', 3 : 'Dress', 6 : 'Shirt'}\n",
    "for label in labels:\n",
    "    print(\"Label: {0}, value: {1}\".format(label, labels_map[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8));\n",
    "columns = 4;\n",
    "rows = 5;\n",
    "for i in range(1, columns*rows +1):\n",
    "    img_xy = np.random.randint(len(train_dataset));\n",
    "    img = train_dataset[img_xy][0][0,:,:]\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.title(labels_map[train_dataset[img_xy][1]])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of code block below\n",
    "\n",
    "For our data pre-processing we have cleaned our data, applied required transformations, as well as wrote a simple API that allows us to do the steps gracefully.\n",
    "What we need now is another API which can help us in iterating over our training/test datasets efficiently.\n",
    "\n",
    "The following code does the same, **DataLoader** class provides us with an *Python* `iterator` object which allows for traverse over our dataset very efficiently, in specific batch size we want.\n",
    "\n",
    "```\n",
    "for i_batch, sample_batched in enumerate(trainloader):\n",
    "    do_something(sample_batched)\n",
    "```\n",
    "\n",
    "Since trainloader is an iterator we can easily now iterate over our data set, you can also the see the code example above.\n",
    "\n",
    "#### What's my intuition behind this ?\n",
    "\n",
    "The most basic intuition in making an iterator here is we need to carry the basic Neural Net operations: \n",
    "  - Forward Propagation \n",
    "  - Backward Propagation\n",
    "\n",
    "Since we want to carry out **Mini Batch Gradient Descent** for our Neural Network to learn the hyper-params, we are using this iterator to automatically create the required batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    We built our data set using DataSetLoader class we defined above, now we need the the following operations\n",
    "    1. Divide our dataset the into the batch sizes.\n",
    "    2. Shuffle the dataset accordingly for randomness.\n",
    "\"\"\"\n",
    "trainloader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
