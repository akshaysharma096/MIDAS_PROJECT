{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter tweets parser - MIDAS Intership Challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation - 1\n",
    "The code in the block below, is added to import 3rd party libaries like **requests** and **pandas** which are very handy and ease out the development process, as well as we use the code block below to import in-built Python libraries like \n",
    "**os** and **json**\n",
    "\n",
    "\n",
    "### What is the code block doing\n",
    "The following code block is importing required in-built/3rd party libraries for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing neccassary 3rd party as well in-built Python libraries\n",
    "import requests\n",
    "import collections\n",
    "import os\n",
    "import oauth2\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation - 2\n",
    "### Intuition\n",
    "The intutition of the code block below is to create a new **Data Type** to solve the problem we have with ease, we can consider the class `MidasTweetParser` as a new **Data Type**, and the methods defined as the state of that data type.\n",
    "    \n",
    "We have used the instance variable *self.tabular_data* as a Pandas Data frame so that we can easily show the data in tabular form as easily possible, with minimum extra code.\n",
    "\n",
    "We have stored our API keys as environment variable so that they are safe.\n",
    "To keep the code more clean and readable we have created an exception class, `TwitterAPIException`, so to handle the case when any of the Twitter APIs fail.\n",
    "\n",
    "To dump data in a JSONlines file, which is basically a file where each JSON object is dumped one by one and each object is delimitted by a `\\n`, we have used *jsonlines* python module for this, though we can also dump data in file by using basic file operations.\n",
    "\n",
    "\n",
    "### What is the code block doing\n",
    "The code block below, incorporates the logic of what needs to be done as far this problem of parsing tweets, dumping them in JSONlines files, as well populating the data frame to show them later in tabular form.\n",
    "\n",
    "As well as we are taking care of cases when our request to Twitter APIs fail, because of any of the reason.\n",
    "`class MidasTweetParser`, has a constructor, the directory in which we should make the *JSONlines* file as well as the filename, default values are folder: **\"Midas_Tweets\"** and file name as **midas_tweets_record** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterAPIException(Exception):\n",
    "    \"\"\"\n",
    "     An exception to handle the case when Twitter's APIs fail.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidasTweetParser:\n",
    "    def __init__(self, directory_name=None, file_name=None):\n",
    "        \"\"\"\n",
    "         This is a constructor for the class, the intuition behind the class it to keep the code, clean\n",
    "         Pandas Dataframe is used so that we can easily show our tweets collection in tabular form.\n",
    "        \"\"\"\n",
    "        self.consumer_key = os.environ[\"TWITTER_ACCESS_TOKEN\"]\n",
    "        self.consumer_secret = os.environ[\"TWITTER_ACCESS_TOKEN_SECRET\"]\n",
    "        \n",
    "        if directory_name:\n",
    "            self.tweets_dir = directory_name\n",
    "        else:\n",
    "            self.tweets_dir = \"Midas_Tweets\"\n",
    "            \n",
    "        if file_name:\n",
    "            # take the first name only\n",
    "            self.file_name = file_name.split(\".\")[0]\n",
    "        else:\n",
    "            self.file_name = \"midas_tweets_record\"\n",
    "            \n",
    "        self.request_token_url = \"https://api.twitter.com/oauth2/token?grant_type=client_credentials\"\n",
    "        self.tweets_url = \"https://api.twitter.com/1.1/statuses/user_timeline.json\"\n",
    "        self.tweets_url_params = {\"screen_name\":\"midasiiitd\", \"page\": 1}\n",
    "        self.tabular_data = pd.DataFrame()\n",
    "    \n",
    "    def _get_bearer_token(self):\n",
    "        \"\"\" \n",
    "         Method to complete the OAuth flow, to get bearer token for the following requests.\n",
    "        \"\"\"\n",
    "        response = requests.post(self.request_token_url, auth=HTTPBasicAuth(self.consumer_key, self.consumer_secret))\n",
    "        if response.status_code != 200:\n",
    "            raise TwitterAPIException(\"{0}, error code: {1}\".format(\"Could not fetch Bearer token from Twitter\", response.status_code))\n",
    "        response_data = json.loads(response.content.decode())\n",
    "        return response_data[\"access_token\"]\n",
    "    \n",
    "    def _handle_tweets(self, tweets):\n",
    "        \"\"\"\n",
    "          The code works to get tweets in batches of 20, to save memory.\n",
    "          This method handles each batch;\n",
    "          \n",
    "          1. Parse each tweet object, to get the required fields.\n",
    "          2. Dump each tweet object into a separate JSON file.\n",
    "          3. Add build a pandas data frame to show the collection of tweets in a tabular form.\n",
    "        \"\"\"\n",
    "        print(\"Handling {0} tweets from page: {1}\".format(len(tweets), self.tweets_url_params[\"page\"]))\n",
    "        for tweet in tweets:\n",
    "            date_time = tweet.get(\"created_at\")\n",
    "            text = tweet.get(\"text\")\n",
    "            favourite_likes = int(tweet.get(\"favorite_count\"))\n",
    "            retweet_count = int(tweet.get(\"retweet_count\"))\n",
    "            entities = tweet.get(\"extended_entities\", None)\n",
    "            images_count = 0\n",
    "            if entities:\n",
    "                images_count = len(entities.get(\"media\", []))\n",
    "\n",
    "            data_frame_row = {\n",
    "                \"Time and Date of Creation\": pd.to_datetime(date_time),\n",
    "                \"Number of Retweets\":retweet_count,\n",
    "                \"Number of Images Present\":images_count,\n",
    "                \"Favourite Count\":favourite_likes,\n",
    "                \"Text\":text,\n",
    "            }\n",
    "            self.tabular_data = self.tabular_data.append(data_frame_row, ignore_index=True)\n",
    "            self._dump_tweet(tweet)\n",
    "            \n",
    "    def _dump_tweet(self, tweet):\n",
    "        \"\"\"\n",
    "         Method to dump tweets into a single file\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.tweets_dir):\n",
    "            os.mkdir(self.tweets_dir)\n",
    "            \n",
    "        tweet_id = tweet.get(\"id\", uuid.uuid4())\n",
    "        path = \"{0}/{1}.jsonl\".format(self.tweets_dir, self.file_name)\n",
    "        \n",
    "        if os.path.exists(path):\n",
    "            mode = 'a' # mode is append to a file if file exists.\n",
    "        else:\n",
    "            mode = 'w' # mode is create a file if file does not exists.\n",
    "        with jsonlines.open(path, mode=mode) as writer:\n",
    "            writer.write(tweet)\n",
    "        \n",
    "    def _build_bearer_token(self, token):\n",
    "        \"\"\"\n",
    "         This code block is used to build the format for the access token that Twitter accepts.\n",
    "         eg: Bearer AADKJKJDKF.....090KKKM09232M\n",
    "        \"\"\"\n",
    "        return \"{0} {1}\".format(\"Bearer\", token)\n",
    "    \n",
    "    def _show_tabular_data(self):\n",
    "        return self.tabular_data\n",
    "    \n",
    "    def _get_total_number_tweets(self):\n",
    "        return self.tabular_data.shape[0]\n",
    "    \n",
    "    def _get_json_for_tweets(self):\n",
    "        \"\"\"\n",
    "         1. This method is useful as it uses the bearer token received via Twitter's OAith APIs to fetch tweets,\n",
    "            for the required twitter handle, 'midasiiitd' for this code problem, in batches of 20.\n",
    "            \n",
    "         2. The method also returns the response of the Twitter API as a Python dictionary.\n",
    "        \"\"\"\n",
    "        bearer_token = self._get_bearer_token()\n",
    "        authorization_header = self._build_bearer_token(bearer_token)\n",
    "        response = requests.get(self.tweets_url,\n",
    "                                headers={\"Authorization\":authorization_header}, params=self.tweets_url_params)\n",
    "        if response.status_code != 200:\n",
    "            raise TwitterAPIException(\"{0}, error code: {1}\".format(\"Could not fetch tweets from Twitter\", response.status_code))\n",
    "        return json.loads(response.content)\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"Starting parser........\")\n",
    "        tweets_data = self._get_json_for_tweets()\n",
    "        while tweets_data:\n",
    "            self._handle_tweets(tweets_data)\n",
    "            self.tweets_url_params[\"page\"] += 1\n",
    "            tweets_data = self._get_json_for_tweets()\n",
    "        \n",
    "        print(\"\\nSuccessfully, populated the table as well as dumped the JSON for each in folder /{0} and file: {1}.jsonl \\n\".format(self.tweets_dir,\n",
    "                                                                                                                              self.file_name))\n",
    "        print(\"-----------------\\nPrinting the tabular data\\n-----------------\")\n",
    "        total_tweets = self._get_total_number_tweets()\n",
    "        print(\"Total number of tweets made by 'midasiiitd', are: {0}\".format(total_tweets))\n",
    "        return self._show_tabular_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the object\n",
    "tweets_parser = MidasTweetParser(directory_name=\"MIDAS_INTERNSIP_PROJECT\", file_name=\"tweets\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parser........\n",
      "Handling 20 tweets from page: 1\n",
      "Handling 20 tweets from page: 2\n",
      "Handling 20 tweets from page: 3\n",
      "Handling 20 tweets from page: 4\n",
      "Handling 20 tweets from page: 5\n",
      "Handling 20 tweets from page: 6\n",
      "Handling 20 tweets from page: 7\n",
      "Handling 20 tweets from page: 8\n",
      "Handling 20 tweets from page: 9\n",
      "Handling 20 tweets from page: 10\n"
     ]
    }
   ],
   "source": [
    "# Run the parser\n",
    "tweets_parser.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As we can see by using a single class we have successfully created a parser which fetches tweets from Twitter, populates them in a tabular form as well as dumps each tweets in a **JSONLINES** file.\n",
    "\n",
    "**By - Akshay Sharma, email: akshay.sharma09695@gmail.com**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
